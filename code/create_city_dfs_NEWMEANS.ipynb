{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "019fd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libaries\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a39359b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# load city df and variables\n",
    "##########################################################################\n",
    "result = pyreadr.read_r('/data/users/laura.owen/extremes/heatwaves/HadUKGrid/dur-clim/coords/UK_top30_cities.Rda')\n",
    "city_df = result['city_df']\n",
    "\n",
    "yrs = list(range(1980, 2081, 2)) #51years\n",
    "RPy = [0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000] #10RPlevels\n",
    "do_dur = list(range(1, 10)) #9 durations\n",
    "stens = [\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\"] #12 ensembles\n",
    "\n",
    "# Lists to store per-city results \n",
    "city_levxally_lists = {}\n",
    "city_nDeqdally_lists = {}\n",
    "city_th_lists = {}\n",
    "city_mhwt_lists = {}\n",
    "\n",
    "#which version of make stat to use?\n",
    "make_stat = 'old' # 'old' or 'new'\n",
    "if make_stat == 'old':\n",
    "    main_path = '/home/users/laura.owen/old-home/data/users/lowen/extremes/heatwaves/HadUKGrid/dur-clim/probs'\n",
    "    output_path = \"../dataframes\"\n",
    "elif make_stat == 'new':\n",
    "    main_path = '/data/users/laura.owen/extremes/heatwaves/HadUKGrid/dur-clim/probs'\n",
    "    output_path = \"../dataframes/new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb4df87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global counter for files\n",
    "missing_files_count = 0\n",
    "found_files_count = 0\n",
    "\n",
    "# Helper function to load .rds or return fallback array\n",
    "def try_read_rds(path, fallback_shape=None):\n",
    "    global missing_files_count, found_files_count\n",
    "    if os.path.exists(path):\n",
    "        found_files_count += 1\n",
    "        result = pyreadr.read_r(path)\n",
    "        return result[None]  # assume single unnamed object\n",
    "    else:\n",
    "        missing_files_count += 1  # Increment the counter for missing files\n",
    "        if fallback_shape:\n",
    "            #print(f\"File not found: {path}, filling with NaNs\")\n",
    "            return np.full(fallback_shape, np.nan)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File not found and no fallback: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd1ab0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing files: 165\n",
      "Total files found: 1275\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Loop over each city to get ensemble probs and ensemble mean probs to store\n",
    "##########################################################################\n",
    "for _, row in city_df.iterrows():\n",
    "    city_name = str(row[\"city\"]).replace(\" \", \"_\")\n",
    "    xco = str(row[\"lon_index\"])\n",
    "    yco = str(row[\"lat_index\"])\n",
    "\n",
    "    # create lists to store data for each ens\n",
    "    levxally_list = {}\n",
    "    nDeqdally_list = {}\n",
    "    th_list = {}\n",
    "    mhwt_list = {}\n",
    "        \n",
    "    ##########################################################################\n",
    "    # read in all ens prob files and store\n",
    "    ##########################################################################\n",
    "    for s in stens:\n",
    "        ens_path = f\"{main_path}/{s}\"\n",
    "        levxally_path = f\"{ens_path}/levxally_{city_name}_{xco}_{yco}_{s}.rds\"\n",
    "        nDeqdally_path = f\"{ens_path}/nDeqdally_{city_name}_{xco}_{yco}_{s}.rds\"\n",
    "        th_path = f\"{ens_path}/tha_{city_name}_{xco}_{yco}_{s}.rds\"\n",
    "        mhwt_path = f\"{ens_path}/mhwt_{city_name}_{xco}_{yco}_{s}.rds\"\n",
    "        \n",
    "        levxally_list[s] = try_read_rds(levxally_path, fallback_shape=(3, 10, 51)) #servity type (severity,mean,peakvalue), RP and years\n",
    "        nDeqdally_list[s] = try_read_rds(nDeqdally_path, fallback_shape=(9, 51)) #duration and years\n",
    "        th_list[s] = try_read_rds(th_path, fallback_shape=(51,))\n",
    "        mhwt_list[s] = try_read_rds(mhwt_path, fallback_shape=(51,)) \n",
    "\n",
    "    city_levxally_lists[city_name] = levxally_list\n",
    "    city_nDeqdally_lists[city_name] = nDeqdally_list\n",
    "    city_th_lists[city_name] = th_list\n",
    "    city_mhwt_lists[city_name] = mhwt_list\n",
    "\n",
    "# Print the total number of missing files\n",
    "print(f\"Total missing files: {missing_files_count}\")\n",
    "print(f\"Total files found: {found_files_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8788de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_161159/4037569345.py:36: RuntimeWarning: Mean of empty slice\n",
      "  mean_thresholds = np.nanmean(all_thresholds, axis=0)\n",
      "/var/tmp/ipykernel_161159/4037569345.py:37: RuntimeWarning: Mean of empty slice\n",
      "  mean_mhwts = np.nanmean(all_mhwts, axis=0)\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# save dictionary of th and mhwt\n",
    "##########################################################################\n",
    "dataframes = {}  # Dictionary to store DataFrames for each city\n",
    "\n",
    "for city_row in city_df.itertuples():\n",
    "    city = str(city_row.city).replace(\" \", \"_\")\n",
    "\n",
    "    th_list = city_th_lists[city]\n",
    "    mhwt_list = city_mhwt_lists[city]\n",
    "\n",
    "    data = []\n",
    "    all_thresholds = []\n",
    "    all_mhwts = []\n",
    "\n",
    "    # Add ensemble members\n",
    "    for ens in stens:\n",
    "        df_th = th_list[ens]\n",
    "        df_mhwt = mhwt_list[ens]\n",
    "\n",
    "        thresholds = df_th.iloc[:, 0].values if isinstance(df_th, pd.DataFrame) else df_th\n",
    "        mhwts = df_mhwt.iloc[:, 0].values if isinstance(df_mhwt, pd.DataFrame) else df_mhwt\n",
    "\n",
    "        all_thresholds.append(thresholds)\n",
    "        all_mhwts.append(mhwts)\n",
    "\n",
    "        for year, threshold, mhwt in zip(yrs, thresholds, mhwts):\n",
    "            data.append({\n",
    "                \"year\": year,\n",
    "                \"threshold\": threshold,\n",
    "                \"mhwt\": mhwt,\n",
    "                \"ensemble\": ens\n",
    "            })\n",
    "\n",
    "    # Compute ensemble means (handle NaNs)\n",
    "    mean_thresholds = np.nanmean(all_thresholds, axis=0)\n",
    "    mean_mhwts = np.nanmean(all_mhwts, axis=0)\n",
    "\n",
    "    for year, threshold, mhwt in zip(yrs, mean_thresholds, mean_mhwts):\n",
    "        data.append({\n",
    "            \"year\": year,\n",
    "            \"threshold\": threshold,\n",
    "            \"mhwt\": mhwt,\n",
    "            \"ensemble\": \"mean\"\n",
    "        })\n",
    "\n",
    "    # Create DataFrame for the current city\n",
    "    df = pd.DataFrame(data)\n",
    "    dataframes[city] = df  # Store the DataFrame in the dictionary\n",
    "\n",
    "# Save the dataframes dictionary to a file\n",
    "with open(f\"{output_path}/th_mhwt_dataframes_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(dataframes, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4cf939fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_161159/4089111302.py:31: RuntimeWarning: Mean of empty slice\n",
      "  mean_nDeqdally = np.nanmean(all_ens_values, axis=0) # shape: [duration, year]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# save dictionary of durations\n",
    "##########################################################################\n",
    "duration_dataframes = {}\n",
    "\n",
    "for city_row in city_df.itertuples():\n",
    "    city = str(city_row.city).replace(\" \", \"_\")\n",
    "\n",
    "    nDeqdally_list = city_nDeqdally_lists[city]  # dict: {ensemble: [duration x year]}\n",
    "    data = []\n",
    "\n",
    "    # Gather values for computing the mean\n",
    "    all_ens_values = []\n",
    "\n",
    "    for ens in stens:\n",
    "        df_dur = nDeqdally_list[ens]\n",
    "        values = df_dur.values if isinstance(df_dur, pd.DataFrame) else df_dur\n",
    "        all_ens_values.append(values)\n",
    "\n",
    "        for dur in range(1, values.shape[0] + 1):\n",
    "            for year_idx, year in enumerate(yrs):\n",
    "                data.append({\n",
    "                    \"city\": city,\n",
    "                    \"duration\": dur,\n",
    "                    \"year\": year,\n",
    "                    \"nduration\": values[dur - 1, year_idx],\n",
    "                    \"ensemble\": ens\n",
    "                })\n",
    "\n",
    "    # Compute mean across ensemble members\n",
    "    mean_nDeqdally = np.nanmean(all_ens_values, axis=0) # shape: [duration, year]\n",
    "\n",
    "    for dur in range(1, mean_nDeqdally.shape[0] + 1):\n",
    "        for year_idx, year in enumerate(yrs):\n",
    "            data.append({\n",
    "                \"city\": city,\n",
    "                \"duration\": dur,\n",
    "                \"year\": year,\n",
    "                \"nduration\": mean_nDeqdally[dur - 1, year_idx],\n",
    "                \"ensemble\": \"mean\"\n",
    "            })\n",
    "\n",
    "    # Create DataFrame for city\n",
    "    df = pd.DataFrame(data)\n",
    "    duration_dataframes[city] = df\n",
    "\n",
    "# Save dictionary of duration-based data\n",
    "with open(f\"{output_path}/duration_dataframes_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(duration_dataframes, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54bc21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_161159/3699491817.py:36: RuntimeWarning: Mean of empty slice\n",
      "  mean_levx = np.nanmean(all_levx_array, axis=0)  # shape: [severity_type, rp_level, year]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# save dictionary of levxally\n",
    "##########################################################################\n",
    "# LEVXALLY(3,10,51) - 3 severity types, 10 RP levels, 51 years\n",
    "# 1) severity 2) mean severity 3) peak value\n",
    "levxally_dataframes = {}\n",
    "\n",
    "for city_row in city_df.itertuples():\n",
    "    city = str(city_row.city).replace(\" \", \"_\")\n",
    "\n",
    "    levxally_list = city_levxally_lists[city]  # dict: {ensemble: [severity x RP x year]}\n",
    "    data = []\n",
    "\n",
    "    # Collect all ensemble arrays\n",
    "    all_levx = []\n",
    "\n",
    "    for ens in stens:\n",
    "        levx = levxally_list[ens]\n",
    "        levx_values = levx.values if isinstance(levx, pd.DataFrame) else levx\n",
    "        all_levx.append(levx_values)\n",
    "\n",
    "        for sev_idx in range(levx_values.shape[0]):\n",
    "            for rp_idx in range(levx_values.shape[1]):\n",
    "                for year_idx, year in enumerate(yrs):\n",
    "                    data.append({\n",
    "                        \"city\": city,\n",
    "                        \"severity_type\": sev_idx + 1,\n",
    "                        \"rp_level\": rp_idx + 1,\n",
    "                        \"year\": year,\n",
    "                        \"severity_value\": levx_values[sev_idx, rp_idx, year_idx].item(),\n",
    "                        \"ensemble\": ens\n",
    "                    })\n",
    "\n",
    "    # Compute ensemble mean\n",
    "    all_levx_array = np.array(all_levx)  # shape: [ensemble, severity_type, rp_level, year]\n",
    "    mean_levx = np.nanmean(all_levx_array, axis=0)  # shape: [severity_type, rp_level, year]\n",
    "\n",
    "    # Add mean values\n",
    "    for sev_idx in range(mean_levx.shape[0]):\n",
    "        for rp_idx in range(mean_levx.shape[1]):\n",
    "            for year_idx, year in enumerate(yrs):\n",
    "                data.append({\n",
    "                    \"city\": city,\n",
    "                    \"severity_type\": sev_idx + 1,\n",
    "                    \"rp_level\": rp_idx + 1,\n",
    "                    \"year\": year,\n",
    "                    \"severity_value\": mean_levx[sev_idx, rp_idx, year_idx].item(),\n",
    "                    \"ensemble\": \"mean\"\n",
    "                })\n",
    "\n",
    "    # Create DataFrame for city\n",
    "    df = pd.DataFrame(data)\n",
    "    levxally_dataframes[city] = df\n",
    "\n",
    "# Save dictionary of severity-based data\n",
    "with open(f\"{output_path}/levxally_dataframes_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(levxally_dataframes, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18ae39df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         city  severity_type  rp_level  year  severity_value ensemble\n",
      "0      London              1         1  1980        0.497086       01\n",
      "1      London              1         1  1982        0.562421       01\n",
      "2      London              1         1  1984        0.640468       01\n",
      "3      London              1         1  1986        0.715464       01\n",
      "4      London              1         1  1988        0.817204       01\n",
      "...       ...            ...       ...   ...             ...      ...\n",
      "19885  London              3        10  2072       45.694823     mean\n",
      "19886  London              3        10  2074       45.877134     mean\n",
      "19887  London              3        10  2076       46.003327     mean\n",
      "19888  London              3        10  2078       46.107225     mean\n",
      "19889  London              3        10  2080       46.234425     mean\n",
      "\n",
      "[19890 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# all_rp_levels = set()\n",
    "\n",
    "# for df in levxally_dataframes.values():\n",
    "#     all_rp_levels.update(df[\"rp_level\"].unique())\n",
    "\n",
    "# rp_levels_present = sorted(all_rp_levels)\n",
    "# print(\"All RP levels present across cities:\", rp_levels_present)\n",
    "\n",
    "# RPy = [0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "# rpys_present = [RPy[i - 1] for i in rp_levels_present]\n",
    "# print(\"All RPy values present:\", rpys_present)\n",
    "\n",
    "print(levxally_dataframes[\"London\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
