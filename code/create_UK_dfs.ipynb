{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70dda7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from pathlib import Path\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "461bb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "fig_path = Path(\"../figures\")\n",
    "df_path = Path(\"../dataframes\")\n",
    "base_path = Path(\"/home/users/laura.owen/old-home/data/users/lowen/extremes/heatwaves/HadUKGrid/dur-clim/probs\")\n",
    "\n",
    "# Load UK coords\n",
    "result = pyreadr.read_r(\"/data/users/laura.owen/extremes/heatwaves/HadUKGrid/dur-clim/coords/UK_even_coords.RData\")\n",
    "lons = result[\"even_xorder_land_lon_indices\"]\n",
    "lats = result[\"even_xorder_land_lat_indices\"]\n",
    "UK_df = pd.DataFrame({\n",
    "    \"lon_index\": lons.values.flatten(),\n",
    "    \"lat_index\": lats.values.flatten()\n",
    "})\n",
    "\n",
    "#take subset for testing\n",
    "#UK_df = UK_df.iloc[0:100, :]\n",
    "\n",
    "# Year and return period info\n",
    "yrs = [1980, 2080]\n",
    "RPy = [0.5, 2, 5, 10, 20, 50, 100, 200, 500, 1000] #10RP levels\n",
    "do_dur = list(range(1, 10)) #9 durations\n",
    "stens = [\"01\"] # currently just have ens 01 for UK wide\n",
    "#stens = [\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\"] #12 ensembles\n",
    "\n",
    "# Empty dataframes to collect results\n",
    "severity_df = []\n",
    "mseverity_df = []\n",
    "peakvalue_df = []\n",
    "duration_df = []\n",
    "tha_df = []\n",
    "mhwt_df= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "583de000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Length mismatch! tha has length 51 at x=146, y=44, st=01\n",
      "⚠️ Length mismatch! tha has length 51 at x=147, y=46, st=01\n",
      "⚠️ Length mismatch! tha has length 51 at x=148, y=44, st=01\n",
      "Levxally files: processed=5106, skipped=92\n",
      "nDeqdally files: processed=5106, skipped=92\n",
      "Tha files: processed=5103, skipped=95\n",
      "Mhwt files: processed=5112, skipped=86\n"
     ]
    }
   ],
   "source": [
    "skipped_levxally = 0\n",
    "skipped_nDeqdally = 0\n",
    "skipped_tha = 0\n",
    "skipped_mhwt = 0\n",
    "processed_levxally = 0\n",
    "processed_nDeqdally = 0\n",
    "processed_tha = 0\n",
    "processed_mhwt = 0\n",
    "\n",
    "for _, row in UK_df.iterrows():\n",
    "    xco = str(row[\"lon_index\"])\n",
    "    yco = str(row[\"lat_index\"])\n",
    "\n",
    "    for st in stens:\n",
    "        levxally_path = base_path / f\"{st}/levxally_point_{xco}_{yco}_{st}.rds\"\n",
    "        nDeqdally_path = base_path / f\"{st}/nDeqdally_point_{xco}_{yco}_{st}.rds\"\n",
    "        tha_path = base_path / f\"{st}/tha_point_{xco}_{yco}_{st}.rds\"\n",
    "        mhwt_path = base_path / f\"{st}/mhwt_point_{xco}_{yco}_{st}.rds\"\n",
    "\n",
    "        if levxally_path.exists():\n",
    "            levxally = pyreadr.read_r(levxally_path)[None]  # shape (3, 10, n_years)\n",
    "            severity_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            mseverity_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            peakvalue_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            expected_years = [1980, 2080]\n",
    "            actual_years = expected_years[:levxally.shape[2]]  \n",
    "            for rp_idx in range(10):\n",
    "                for year in expected_years:\n",
    "                    col_name = f\"rp{rp_idx+1}_{year}\"\n",
    "                    severity_dict[col_name] = np.nan\n",
    "                    mseverity_dict[col_name] = np.nan\n",
    "                    peakvalue_dict[col_name] = np.nan\n",
    "            for rp_idx in range(10):\n",
    "                for year_idx, year in enumerate(actual_years):\n",
    "                    col_name = f\"rp{rp_idx+1}_{year}\"\n",
    "                    severity_dict[col_name] = levxally[0, rp_idx, year_idx].item()\n",
    "                    mseverity_dict[col_name] = levxally[1, rp_idx, year_idx].item()\n",
    "                    peakvalue_dict[col_name] = levxally[2, rp_idx, year_idx].item()\n",
    "            severity_df.append(severity_dict)\n",
    "            mseverity_df.append(mseverity_dict)\n",
    "            peakvalue_df.append(peakvalue_dict)\n",
    "            processed_levxally += 1\n",
    "        else:\n",
    "            skipped_levxally += 1\n",
    "\n",
    "        if nDeqdally_path.exists():\n",
    "            nDeqdally = pyreadr.read_r(nDeqdally_path)[None]  # Expected shape (durations, years)\n",
    "            duration_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            for dur_idx, dur in enumerate(do_dur):\n",
    "                for year in yrs:\n",
    "                    col_name = f\"duration{dur}_{year}\"\n",
    "                    duration_dict[col_name] = np.nan\n",
    "            n_dur = nDeqdally.shape[0]\n",
    "            n_years = nDeqdally.shape[1]\n",
    "            for dur_idx in range(n_dur):\n",
    "                for year_idx in range(n_years):\n",
    "                    if dur_idx < len(do_dur) and year_idx < len(yrs):\n",
    "                        col_name = f\"duration{do_dur[dur_idx]}_{yrs[year_idx]}\"\n",
    "                        try:\n",
    "                            duration_dict[col_name] = nDeqdally.iloc[dur_idx, year_idx]\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error reading nDeqdally at dur_idx={dur_idx}, year_idx={year_idx}: {e}\")\n",
    "            duration_df.append(duration_dict)\n",
    "            processed_nDeqdally += 1\n",
    "        else:\n",
    "            skipped_nDeqdally += 1\n",
    "\n",
    "        if tha_path.exists():\n",
    "            tha = pyreadr.read_r(tha_path)[None].squeeze()\n",
    "            tha_years = [1980, 2020, 2080]\n",
    "            tha_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            for year in tha_years:\n",
    "                tha_dict[f\"tha{year}\"] = np.nan\n",
    "            if len(tha) == 3:\n",
    "                for idx, year in enumerate(tha_years):\n",
    "                    tha_dict[f\"tha{year}\"] = tha.iloc[idx]\n",
    "                processed_tha += 1\n",
    "            elif len(tha) == 2:\n",
    "                tha_dict[f\"tha1980\"] = tha.iloc[0]\n",
    "                tha_dict[f\"tha2080\"] = tha.iloc[1]\n",
    "                processed_tha += 1\n",
    "            else:\n",
    "                print(f\"⚠️ Length mismatch! tha has length {len(tha)} at x={xco}, y={yco}, st={st}\")\n",
    "                skipped_tha += 1\n",
    "\n",
    "            tha_df.append(tha_dict)\n",
    "        else:\n",
    "            skipped_tha += 1\n",
    "\n",
    "        if mhwt_path.exists():\n",
    "            mhwt = pyreadr.read_r(mhwt_path)[None].squeeze()\n",
    "            mhwt_years = [1980, 2020, 2080]\n",
    "            mhwt_dict = {\"xco\": xco, \"yco\": yco, \"ensemble\": st}\n",
    "            for year in mhwt_years:\n",
    "                mhwt_dict[f\"mhwt{year}\"] = np.nan\n",
    "            for idx in range(len(mhwt)):\n",
    "                if idx < len(mhwt_years):\n",
    "                    mhwt_dict[f\"mhwt{mhwt_years[idx]}\"] = mhwt.iloc[idx]\n",
    "                else:\n",
    "                    print(f\"⚠️ Extra data in mhwt beyond expected years at x={xco}, y={yco}, st={st}\")\n",
    "            mhwt_df.append(mhwt_dict)\n",
    "            processed_mhwt += 1\n",
    "        else:\n",
    "            skipped_mhwt += 1\n",
    "\n",
    "print(f\"Levxally files: processed={processed_levxally}, skipped={skipped_levxally}\")\n",
    "print(f\"nDeqdally files: processed={processed_nDeqdally}, skipped={skipped_nDeqdally}\")\n",
    "print(f\"Tha files: processed={processed_tha}, skipped={skipped_tha}\")\n",
    "print(f\"Mhwt files: processed={processed_mhwt}, skipped={skipped_mhwt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bf526e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrames\n",
    "severity_df = pd.DataFrame(severity_df)\n",
    "mseverity_df = pd.DataFrame(mseverity_df)\n",
    "peakvalue_df = pd.DataFrame(peakvalue_df)\n",
    "duration_df = pd.DataFrame(duration_df)\n",
    "tha_df = pd.DataFrame(tha_df)\n",
    "mhwt_df = pd.DataFrame(mhwt_df)\n",
    "\n",
    "# print(severity_df)\n",
    "# print(duration_df)\n",
    "# print(tha_df)\n",
    "# print(mhwt_df)\n",
    "\n",
    "#save as pandas dfs\n",
    "severity_df.to_csv(df_path / \"severity_df.csv\", index=False)\n",
    "mseverity_df.to_csv(df_path / \"mseverity_df.csv\", index=False)\n",
    "peakvalue_df.to_csv(df_path / \"peakvalue_df.csv\", index=False)\n",
    "duration_df.to_csv(df_path / \"duration_df.csv\", index=False)\n",
    "tha_df.to_csv(df_path / \"tha_df.csv\", index=False)\n",
    "mhwt_df.to_csv(df_path / \"mhwt_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
